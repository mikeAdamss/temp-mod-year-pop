{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOMIS API Data Extract for Historic Census Data.\n",
    "\n",
    "The NOMIS RESTful API limits the size of the 'queries / calls' you can request.\n",
    "For example, you cannot request all of the geography codes in a single call, the site will time-out or error due to the number of data items. Additionally each call is limited to a maximum of 25k records; hence the need for a few nested programming loops etc. etc. etc.\n",
    "\n",
    "***Here are is a useful reading resource:***\n",
    "https://www.nomisweb.co.uk/api/v01/help\n",
    "\n",
    "***Here is the target data Darren Barnes has requested:***\n",
    "\n",
    "\"*Had a chat with census team this morning and this is the dataset they would like us to test out\n",
    "Its a 2011 census table LC3409EW >> https://www.nomisweb.co.uk/census/2011/lc3409ew and includes General Health, Tenure and Age, covers England and Wales, and goes down to OA level.\n",
    "obviously its available through the NOMIS API and i would like us to suck that into COGS rather than download and transform. The census team have had some very good feedback from Ahmed on the data viz and sparql and i think this has generated some buzz within the census senior management team. So the idea is to load this data into COGS and have Ahmed do a data viz on it and compare that with the CMD.\n",
    "We have looked at NOMIS API already and i would like us to give consideration on what and when we could get this loaded into COGS.\"*\n",
    "\n",
    "**OA**: Census **O**utput **A**reas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components & Libraries:\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import numpy as np\n",
    "from gssutils import pathify\n",
    "\n",
    "from cachecontrol import CacheControl\n",
    "from cachecontrol.caches import FileCache\n",
    "from cachecontrol.heuristics import ExpiresAfter\n",
    "from requests import Session\n",
    "session = CacheControl(Session(), cache=FileCache('.cache'), heuristic=ExpiresAfter(days=7))\n",
    "\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "absolute_start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NM_2010_1\n",
    "str_dataset_id = \"NM_2010_1\"\n",
    "\n",
    "print(str_dataset_id)\n",
    "print('https://www.nomisweb.co.uk/api/v01/dataset/NM_2010_1/geography.def.sdmx.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOMIS RESTful: Get Parent Level Geography for Dataset:\n",
    "\n",
    "# Get Geography sets using our ID:\n",
    "baseURL_Geography = 'https://www.nomisweb.co.uk/api/v01/dataset/' + str_dataset_id + '/geography.def.sdmx.json'\n",
    "df_Geography = pd.read_json(baseURL_Geography)\n",
    "\n",
    "# Not sure if we want more than one geographies for this, but sooner or later we will do so \n",
    "# we'll pretend that we do for now\n",
    "geographies_we_want = [\"2011 output areas\"]\n",
    "geographies_found = []\n",
    "\n",
    "for a_code in df_Geography[\"structure\"]['codelists']['codelist'][0]['code']:\n",
    "\n",
    "    if 'description' not in a_code.keys():\n",
    "        continue\n",
    "        \n",
    "    geog_found_name = a_code['description']['value']\n",
    "    \n",
    "    for geography_we_want in geographies_we_want:\n",
    "        if geog_found_name == geography_we_want:\n",
    "            # Take the whole code dict, never know what we'll need later\n",
    "            geographies_found.append(a_code)\n",
    "                 \n",
    "# Sanity check\n",
    "assert len(geographies_found) == len(geographies_we_want), \\\n",
    "    \"Aborting, we're missing geographies. Wanted {}, got {}.\".format(json.dumps(geographies_we_want, indent=2),\n",
    "                                                                        json.dumps(geographies_found, indent=2))\n",
    "\n",
    "df_Geography = pd.DataFrame() # Memory\n",
    "\n",
    "print(\"Geographies selected: {}\".format(json.dumps(geographies_found, indent=2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get all the dependant codes for these geographies\n",
    "codes_what_we_want = []\n",
    "for a_parent_code_dict in geographies_found:\n",
    "\n",
    "    base_geography_url = 'https://www.nomisweb.co.uk/api/v01/dataset/' + str_dataset_id + '/geography/' + a_parent_code_dict[\"value\"] + '.def.sdmx.json'\n",
    "    base_geography_dict = session.get(base_geography_url).json()\n",
    "    \n",
    "    assert len(base_geography_dict[\"structure\"][\"codelists\"]) == 1, \"We should only have one codelists being\" \\\n",
    "                \" returned from this call.\"\n",
    "    \n",
    "    for a_child_code_dict in base_geography_dict[\"structure\"][\"codelists\"][\"codelist\"][0][\"code\"]:\n",
    "        \"\"\"\n",
    "        this is one, I'm taking the 9 digit geography code to save us a job later\n",
    "        -----\n",
    "        {\n",
    "        \"annotations\": {\n",
    "            \"annotation\": [\n",
    "                {\n",
    "                    \"annotationtext\": \"2011 output areas\",\n",
    "                    \"annotationtitle\": \"TypeName\"\n",
    "                },\n",
    "                {\n",
    "                    \"annotationtext\": 299,\n",
    "                    \"annotationtitle\": \"TypeCode\"\n",
    "                },\n",
    "                {\n",
    "                    \"annotationtext\": \"E00174208\",\n",
    "                    \"annotationtitle\": \"GeogCode\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"parentcode\": 1228931073,\n",
    "        \"description\": {\n",
    "            \"value\": \"E00174208\",\n",
    "            \"lang\": \"en\"\n",
    "        },\n",
    "        \"value\": 1254265842\n",
    "        },\n",
    "        \"\"\"\n",
    "        try:\n",
    "            area = a_child_code_dict[\"annotations\"][\"annotation\"][0][\"annotationtext\"]\n",
    "            if area != \"2011 output areas\":\n",
    "                raise Exception(\"This is supposed to be 2011 output areas, got {}.\".format(area))\n",
    "            \n",
    "            code = a_child_code_dict[\"description\"][\"value\"]\n",
    "            #code = a_child_code_dict[\"value\"]\n",
    "            \n",
    "            codes_what_we_want.append(code)\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Failed on\", json.dumps(a_child_code_dict)) from e\n",
    "\n",
    "print(\"Unique codes identified: \", len(set(codes_what_we_want)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check they seem to be in some sort of order\n",
    "codes_what_we_want.sort()\n",
    "print(codes_what_we_want[:5])\n",
    "print(codes_what_we_want[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fields that we want\n",
    "fields_we_want = [\"DATE\",\"DATE_NAME\",\"DATE_CODE\",\"DATE_TYPE\",\"DATE_TYPECODE\",\"DATE_SORTORDER\",\n",
    "                  \"GEOGRAPHY\",\"GEOGRAPHY_NAME\",\"GEOGRAPHY_CODE\",\"GEOGRAPHY_TYPE\",\"GEOGRAPHY_TYPECODE\",\n",
    "                  \"GEOGRAPHY_SORTORDER\",\"GENDER\",\"GENDER_NAME\",\"GENDER_CODE\",\"GENDER_TYPE\",\n",
    "                  \"GENDER_TYPECODE\",\"GENDER_SORTORDER\",\"C_AGE\",\"C_AGE_NAME\",\"C_AGE_CODE\",\"C_AGE_TYPE\",\n",
    "                  \"C_AGE_TYPECODE\",\"C_AGE_SORTORDER\",\"MEASURES\",\"MEASURES_NAME\",\"OBS_VALUE\",\"OBS_STATUS\",\n",
    "                  \"OBS_STATUS_NAME\",\"OBS_CONF\",\"OBS_CONF_NAME\",\"URN\",\"RECORD_OFFSET\",\"RECORD_COUNT\"]\n",
    "\n",
    "fields_we_want_query_str = \",\".join(fields_we_want)\n",
    "fields_we_want_query_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SubSets\n",
    "\n",
    "I'm going to just grab a subset of 10 codes otherwise this will end in madness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# IMPORTANT - hard coding a smaller subset to try it out\n",
    "codes_what_we_want = [str(x) for x in codes_what_we_want]\n",
    "\n",
    "count = 0\n",
    "end = 99999999999999999 # for now\n",
    "increment = 10\n",
    "endnextloop = False\n",
    "got_since_flush = 0\n",
    "output_counter = 0\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "while True:\n",
    "\n",
    "    if endnextloop:\n",
    "        break\n",
    "        \n",
    "    #print(\"Count {}, Increment {}, end {}\".format(count, increment, end))\n",
    "    \n",
    "    # Get codes by finite increments then finish with whatever the leftovers are\n",
    "    if count+increment > len(codes_what_we_want):\n",
    "        start = count\n",
    "        end = (len(codes_what_we_want)-count)+count\n",
    "        endnextloop = True\n",
    "    else:\n",
    "        start = count\n",
    "        end = count+increment\n",
    "        count = end\n",
    "        endnextloop = False\n",
    "        \n",
    "    got_since_flush += increment\n",
    "        \n",
    "    codes_to_get = \",\".join(codes_what_we_want[start:end])\n",
    "    if len(codes_to_get) == 0:\n",
    "        break\n",
    "        \n",
    "    #print(\"Getting codes: \", codes_to_get)\n",
    "    \n",
    "    #import sys\n",
    "    #sys.exit(1)\n",
    "    \n",
    "    data_url = 'https://www.nomisweb.co.uk/api/v01/dataset/' + str_dataset_id + '.data.csv?date=latest&geography=' + codes_to_get + \"&select=\" + fields_we_want_query_str\n",
    "    #print(\"Trying url {}\".format(data_url))\n",
    "\n",
    "    stream = session.get(data_url, stream=True).raw\n",
    "    stream.decode_content = True\n",
    "    df = pd.read_csv(stream)\n",
    "\n",
    "    additionalURL = ''\n",
    "    intRecordController = 0\n",
    "\n",
    "    # The links below are limited to the first 25,000 cells per call.\n",
    "    while True:\n",
    "        start = datetime.datetime.now()\n",
    "        additionalURL = '&RecordOffset=' + str(intRecordController)\n",
    "        concatenatedURL = data_url + additionalURL\n",
    "        stream = session.get(concatenatedURL, stream=True).raw\n",
    "        stream.decode_content = True\n",
    "\n",
    "        dataframe = pd.read_csv(stream, engine='c', na_filter=False)\n",
    "\n",
    "        if (dataframe.empty):\n",
    "            break\n",
    "\n",
    "        # Additional code due to large datasets:\n",
    "        # For a very basic progress monitor\n",
    "        print(\"Just received record:\" + (dataframe.tail(1).RECORD_OFFSET.to_string(index=False)) + ' of ' + (dataframe.tail(1).RECORD_COUNT.to_string(index=False)) + ' >>> ' + str(round((int(dataframe.tail(1).RECORD_OFFSET.to_string(index=False)) / int(dataframe.tail(1).RECORD_COUNT.to_string(index=False)) * 100),2)) + '%... in ' + str(datetime.datetime.now() - start))\n",
    "        frames = [final_df, dataframe]\n",
    "        final_df = pd.concat(frames)\n",
    "\n",
    "        intRecordController = intRecordController + 25000\n",
    "        \n",
    "    # Going to write and flush every 1000 codes just in case\n",
    "    if got_since_flush > 1000:\n",
    "        df.to_csv(\"./out/output{}.csv\".format(str(output_counter)), index=False)\n",
    "        final_df = pd.DataFrame()\n",
    "        got_since_flush = 0\n",
    "        output_counter +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('final_df.csv.gz', date_format='%Y%m%d', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Completion time for complete script is: \", datetime.datetime.now() - absolute_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
